{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pvc-math/Small-Language-Model/blob/main/Pig_Latin_SLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW-QXyz1fdc-"
      },
      "source": [
        "#Transformers and Language Modeling\n",
        "In this exercise you will implement a Transformer model and several variants such as Encoder Transformers, Decoder Transformers, and Encoder-Decoder transformers.\n",
        "\n",
        "You will then use these as the basis to train a (small) Language Model from scratch on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ErpQkryF1XC",
        "outputId": "2c956dee-ce12-4dd8-cf0e-d9d5407d368d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.10.1 sacrebleu-2.4.2\n",
            "Cloning into 'gtGPT'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 10 (delta 1), reused 10 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (10/10), 7.90 KiB | 7.90 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ],
      "source": [
        "#@title Colab Setup\n",
        "!pip install datasets\n",
        "!pip install tokenizers\n",
        "!pip install sacrebleu\n",
        "!rm -rf gtGPT/\n",
        "!rm -rf gtgpt\n",
        "!git clone https://github.com/Helw150/gtGPT gtGPT\n",
        "!mv gtGPT/gtgpt/ .\n",
        "\n",
        "from gtgpt.utils import set_seed\n",
        "\n",
        "set_seed(3407)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KhpsJ38gF5Pf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":16:8\"\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from gtgpt.model import DummyMultiHeadedSelfAttention, DummyBlock, DummyTransformer, DummyEmbedding\n",
        "from gtgpt.utils import set_seed\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.set_default_device(DEVICE)\n",
        "\n",
        "#Do not change, it will break the AutoGrader\n",
        "setup_block = In[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsNegD_ghJoh"
      },
      "source": [
        "#### Embeddings\n",
        "\n",
        "We will first format our input embeddings similarly to how they are constructed in [BERT](https://arxiv.org/pdf/1810.04805.pdf).\n",
        "\n",
        "Recall from lecture that unlike a RNN, a Transformer does not inherently capture positional information in the forward pass. Because of this, we need to add a signal which encodes the position of each token in its embedding.\n",
        "\n",
        "Your first task is to implement the embedding lookup, including the addition of positional encodings. We have already provided the neccesary parameters inside of `DummyEmbedding`.\n",
        "\n",
        "```python\n",
        "self.vocab_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "self.position_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3J4LRVnqF_pT"
      },
      "outputs": [],
      "source": [
        "class Embedding(DummyEmbedding):\n",
        "    def forward(self, idx):\n",
        "        \"\"\"\n",
        "        :param idx: intTensor of shape (B,T)\n",
        "        :returns embeddings: floatTensor of shape (B,T,n_embd)\n",
        "        \"\"\"\n",
        "        B, T = idx.size()\n",
        "        embeddings = None\n",
        "        #############################################################################\n",
        "        # TODO:\n",
        "        # Implement the embedding lookup.                                           #\n",
        "        #                                                                           #\n",
        "        # This will take a few lines.                                               #\n",
        "        #############################################################################\n",
        "        # use input tensor of token indicies to get corresponding tensor of integer token representation of words\n",
        "        token_embeddings = self.vocab_embeddings(idx)\n",
        "\n",
        "        # Create 1-D tensor of position indicies\n",
        "        positions_idx = torch.arange(T)\n",
        "\n",
        "        # use tensor of position indicies to get corresponding tensor of integer token representation of positions\n",
        "        position_embeddings = self.position_embeddings(positions_idx)\n",
        "\n",
        "        # https://arxiv.org/pdf/1810.04805 tells us to summate the token embeddings and position embeddings\n",
        "        # so that the model understanda the context and order of tokens within a sequence\n",
        "        embeddings = token_embeddings + position_embeddings\n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "        return embeddings\n",
        "\n",
        "#Do not change, it will break the AutoGrader\n",
        "embedding_def = In[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "bFEN1m6jGeDO"
      },
      "outputs": [],
      "source": [
        "#@title Basic Embedding Test\n",
        "\n",
        "def test_embedding():\n",
        "  config = DummyTransformer.get_default_config()\n",
        "  config.vocab_size = 10\n",
        "  config.block_size = 10\n",
        "  config.n_embd = 1\n",
        "  torch.set_default_device(\"cpu\")\n",
        "  set_seed(3047)\n",
        "  embedding = Embedding(config)\n",
        "  embedding.vocab_embeddings.weight = torch.nn.Parameter(torch.arange(0, 10, dtype=torch.float).reshape(10, 1))\n",
        "  embedding.position_embeddings.weight = torch.nn.Parameter(torch.arange(0, 10, dtype=torch.float).reshape(10, 1))\n",
        "  assert torch.allclose(embedding(torch.tensor([[1, 2, 3]])), torch.tensor([1, 3, 5], dtype=torch.float).reshape(1, 3, 1))\n",
        "\n",
        "test_embedding()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N4g9AtKi1Es"
      },
      "source": [
        "#### 3.2 Multi-head Self-Attention\n",
        "Attention can be computed in matrix-form using the following formula:\n",
        "\n",
        "$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
        "\n",
        "We want to have multiple self-attention operations. Each of these is called a head with each head applied to some portion of the input.\n",
        "\n",
        "$head_i = Attention(W_Q X_i, W_K X_i, W_V X_i)$\n",
        "\n",
        "Here, we'll use GPT-style Multi-headed Self-Attention which fragments the head into pieces and applies one head to each fragment. The fragments are then concatenated together to reconstruct the transformed input and projected with a feed-forward layer.\n",
        "\n",
        "$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$\n",
        "\n",
        "Note that while this is \"Multi-head\", all heads can be computed in parallel with a single matrix multiplication. You can find an in-depth description of this in the reference linked in the code.\n",
        "\n",
        "\n",
        "We provide the needed weights in `DummyMultiHeadedSelfAttention`\n",
        "\n",
        "```python\n",
        "# Note that we need this to be true in GPT-style MHA\n",
        "# Knowing this might come in handy :)\n",
        "assert config.n_embd % config.n_head == 0\n",
        "\n",
        "# Note: These could be a single batched linear layer\n",
        "# but we separate them for simplicity of implementation.\n",
        "self.k = nn.Linear(config.n_embd, config.n_embd)\n",
        "self.v = nn.Linear(config.n_embd, config.n_embd)\n",
        "self.q = nn.Linear(config.n_embd, config.n_embd)\n",
        "# output projection\n",
        "self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "# regularization\n",
        "self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "self.hidden_dropout = nn.Dropout(config.hidden_pdrop)\n",
        "\n",
        "self.n_head = config.n_head\n",
        "self.n_embd = config.n_embd\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wJNOQQgaGip4"
      },
      "outputs": [],
      "source": [
        "class GenericSelfAttention(DummyMultiHeadedSelfAttention):\n",
        "    def forward(self, x, attention_mask):\n",
        "        \"\"\"\n",
        "        :param x: float Tensor of shape (batch size, sequence length, embedding dimensionality)\n",
        "        :param attention_mask: int Tensor of shape (batch size, 1, sequence length, sequence_length)\n",
        "        :returns y: float Tensor of shape (batch size, sequence length, embedding dimensionality)\n",
        "        \"\"\"\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        y = None\n",
        "\n",
        "        #############################################################################\n",
        "        # TODO:                                                                     #\n",
        "        # Implement multi-headed self-attention in GPT-2 Style                      #\n",
        "        # Use the provided layers initialized in the DummySelfAttention constructor #\n",
        "        # Apply dropout to the attention values after softmax and the final output  #\n",
        "        #                                                                           #\n",
        "        # Reference:                                                                #\n",
        "        # https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention\n",
        "        #                                                                           #\n",
        "        # Note: All heads should be computed in parallel using the q,k,v layers     #\n",
        "        #                                                                           #\n",
        "        # For each item in the batch, if attention_mask[b, i, j] = 0,               #\n",
        "        # then you should manually set the attention from token i to j to be -inf   #\n",
        "        # Hint: See torch.masked_fill                                               #\n",
        "        #############################################################################\n",
        "        # perform linear transformation on x to get query tensor of shape (B, T, C)\n",
        "        query = self.q(x)\n",
        "\n",
        "        # perform linear transformation on x to get key tensor of shape (B, T, C)\n",
        "        key = self.k(x)\n",
        "\n",
        "        # perform linear transformation on x to get value tensor of shape (B, T, C)\n",
        "        value = self.v(x)\n",
        "\n",
        "        # want dimensionality of q, k, and v vectors to be the\n",
        "        # dimensionality of the embedding vectors divided by the number of heads\n",
        "        qkv_dim = self.n_embd // self.n_head\n",
        "\n",
        "        # update shape\n",
        "        new_shape = (B, T, self.n_head, qkv_dim)\n",
        "\n",
        "        # reshape query tensor to new shape (B, T, number of heads, qkv_dim)\n",
        "        query = torch.reshape(query, new_shape)\n",
        "\n",
        "        # reshape key tensor to new shape (B, T, number of heads, qkv_dim)\n",
        "        key = torch.reshape(key, new_shape)\n",
        "\n",
        "        # reshape value tensor to new shape (B, T, number of heads, qkv_dim)\n",
        "        value = torch.reshape(value, new_shape)\n",
        "\n",
        "        # transpose query tensor to new shape (B, number of heads, T, qkv_dim)\n",
        "        query = torch.transpose(query, 1, 2)\n",
        "\n",
        "        # transpose key tensor to new shape (B, number of heads, T, qkv_dim)\n",
        "        key = torch.transpose(key, 1, 2)\n",
        "\n",
        "        # transpose value tensor to new shape (B, number of heads, T, qkv_dim)\n",
        "        value = torch.transpose(value, 1, 2)\n",
        "\n",
        "        # transpose key tensor to have shape (B, number of heads, qkv_dim, T)\n",
        "        key_transpose = torch.transpose(key, 2, 3)\n",
        "\n",
        "        # compute similarity scores matrix tensor using formula (Q * K^T)/sqrt(d), d = dimension of q,k,v vectors\n",
        "        similarity_scores = torch.matmul(query, key_transpose) / (qkv_dim ** 0.5)\n",
        "\n",
        "        # Masked Self-Attention: Set future scores as 0 so model can't look at future words\n",
        "        # Replace values on upper right of scores matrix with \"-inf\"\n",
        "        masked_attention_scores = similarity_scores.masked_fill(attention_mask == 0, float('-inf'))\n",
        "\n",
        "        # apply softmax to scores along last dimension of (B, self.n_head, T, T)\n",
        "        scores = torch.nn.functional.softmax(masked_attention_scores, 3)\n",
        "\n",
        "        # apply dropout\n",
        "        scores = self.attn_dropout(scores)\n",
        "\n",
        "        # Complete attention computation by multiplying v: # Attention(Q,K,V) = softmax(QK^T/sqrt(d_k))*V\n",
        "        attention = torch.matmul(scores, value)\n",
        "\n",
        "        # transpose attention tensor to have shape (B, T, n_head, qkv_dim)\n",
        "        attention = torch.transpose(attention, 1, 2)\n",
        "\n",
        "        # reshape attention tensor to have shape (B, T, n_embd)\n",
        "        attention = torch.reshape(attention, (B, T, self.n_embd))\n",
        "\n",
        "        # apply linear projection\n",
        "        attention = self.c_proj(attention)\n",
        "\n",
        "        # apply dropout\n",
        "        y = self.hidden_dropout(attention)\n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "\n",
        "        return y\n",
        "\n",
        "#Do not change, it will break the AutoGrader\n",
        "mha_def = In[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOONbP93GrGT",
        "outputId": "9cc148ed-caa5-440d-9ac6-e0e5d4b0f04b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success 1\n",
            "Success 2\n"
          ]
        }
      ],
      "source": [
        "#@title Test Multi-Headed Attention\n",
        "\n",
        "def test_mha():\n",
        "  config = DummyTransformer.get_default_config()\n",
        "  config.vocab_size = 10\n",
        "  config.block_size = 10\n",
        "  config.n_embd = 4\n",
        "  config.n_head = 2\n",
        "  config.hidden_pdrop = 0.25\n",
        "  config.attn_pdrop = 0.1\n",
        "  set_seed(3407)\n",
        "  torch.set_default_device(\"cpu\")\n",
        "  attn = GenericSelfAttention(config)\n",
        "  attn.q.weight = torch.nn.Parameter(torch.eye(2, 2).repeat(2, 2).flip(0))\n",
        "  attn.q.bias = torch.nn.Parameter(torch.zeros(4))\n",
        "  attn.k.weight = torch.nn.Parameter(torch.eye(2, 2).repeat(2, 2))\n",
        "  attn.k.bias = torch.nn.Parameter(torch.zeros(4))\n",
        "  attn.v.weight = torch.nn.Parameter(torch.eye(4, 4))\n",
        "  attn.v.bias = torch.nn.Parameter(torch.zeros(4))\n",
        "  attn.c_proj.weight = torch.nn.Parameter(torch.eye(4, 4))\n",
        "  attn.c_proj.bias = torch.nn.Parameter(torch.zeros(4))\n",
        "  embeddings = torch.tensor([[[1, 2, 3, 4] ,[4, 3, 2, 1]]], dtype=torch.float)\n",
        "  mask = torch.ones(1, 2, 2)\n",
        "  assert torch.allclose(attn(embeddings, mask), torch.tensor([[[5.6779, 0, 0, 0], [0, 3.0456, 4.3618, 5.6779]]], dtype=torch.float), atol=1e-3, rtol=1)\n",
        "  print(\"Success 1\")\n",
        "\n",
        "test_mha()\n",
        "\n",
        "def test_mha_mask():\n",
        "  config = DummyTransformer.get_default_config()\n",
        "  config.vocab_size = 10\n",
        "  config.block_size = 10\n",
        "  config.n_embd = 4\n",
        "  config.n_head = 2\n",
        "  config.hidden_pdrop = 0.0\n",
        "  config.attn_pdrop = 0.0\n",
        "  set_seed(3407)\n",
        "  torch.set_default_device(\"cpu\")\n",
        "  attn = GenericSelfAttention(config)\n",
        "  attn.v.weight = torch.nn.Parameter(torch.eye(4, 4))\n",
        "  attn.v.bias = torch.nn.Parameter(torch.zeros(4))\n",
        "  attn.c_proj.weight = torch.nn.Parameter(torch.eye(4, 4))\n",
        "  attn.c_proj.bias = torch.nn.Parameter(torch.zeros(4))\n",
        "  embeddings = torch.tensor([[[1, 2, 3, 4] ,[4, 3, 2, 1]]], dtype=torch.float)\n",
        "  mask = torch.zeros(1, 2, 2)\n",
        "  mask[0, 0, 0] = 1\n",
        "  mask[0, 1, 1] = 1\n",
        "  assert torch.allclose(attn(embeddings, mask), torch.tensor([[[1, 2, 3, 4], [4, 3, 2, 1]]], dtype=torch.float), atol=1e-4)\n",
        "  print(\"Success 2\")\n",
        "\n",
        "test_mha_mask()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V-LkiiMDG0iN"
      },
      "outputs": [],
      "source": [
        "#@title Now, we can very simply create a single layer transformer block!\n",
        "class TransformerBlock(DummyBlock):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config, GenericSelfAttention)\n",
        "\n",
        "    # A Basic Transformer Block with Attention followed by an MLP\n",
        "    # note the layer norms and residual information preserved at each step.\n",
        "    def forward(self, x, attention_mask):\n",
        "        x = x + self.attn(self.ln_1(x), attention_mask)\n",
        "        x = x + self.mlpf(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "#Do not change, it will break the AutoGrader\n",
        "block_def = In[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XlolMIhnfBa"
      },
      "source": [
        "#### Putting it all together\n",
        "\n",
        "Using our Embedding Layer, the above Transformer Block using our Multi-head attention, and a simple classification head we have all the pieces we need for a Transformer language model.\n",
        "\n",
        "For the forward pass, you'll want to first embed our inputs, apply each transformer layer sequentially, and finally get logits for each possible output word using a classification layer (often called a language modeling head).\n",
        "\n",
        "If an argument is passed to `hidden_cache`, you should prepend it to your input embeddings and pass it alongside the embeddings for the rest of the model. This will allow use to use this structure in Encoder-Decoder architectures later, but also allows passing vectors from any other neural network (such as a computer vision model or an audio model to enable multi-modal understanding). You can find a rich description of how these pieces come together [here](https://jalammar.github.io/illustrated-transformer/).\n",
        "\n",
        "All the parameters you'll need come from `DummyTransformer` and the code blocks above your code section.\n",
        "\n",
        "```python\n",
        "self.transformer = nn.ModuleDict(\n",
        "    dict(\n",
        "        embedding=embedding(config),\n",
        "        h=nn.ModuleList(\n",
        "            [block(config) for _ in range(config.n_layer)]\n",
        "        ),\n",
        "        ln_f=nn.LayerNorm(config.n_embd),\n",
        "    )\n",
        ")\n",
        "self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UJCBDuLzncKd"
      },
      "outputs": [],
      "source": [
        "class GenericTransformer(DummyTransformer):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config, TransformerBlock, Embedding)\n",
        "        self.block_size = config.block_size # Maximum Number of Tokens which can be encoded at once\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "    def get_attention_mask(self, num_tokens):\n",
        "        \"\"\"\n",
        "        Dummy For now, we will see how we use this later!\n",
        "        \"\"\"\n",
        "        B = num_tokens.shape[0]\n",
        "        return torch.ones((B, self.block_size, self.block_size))[:, :num_tokens.max().item(), :num_tokens.max().item()]\n",
        "\n",
        "    def forward(self, idx, targets=None, hidden_cache=None, return_hidden=False):\n",
        "        \"\"\"\n",
        "        :param idx: int Tensor of shape (B,T)\n",
        "        :param hidden_cache: float Tensor of shape (B,P_T,n_embd)\n",
        "        :param targets: int Tensor of shape (B,T_T)\n",
        "        :param return_hidden: bool\n",
        "        (if return_hidden = None)\n",
        "        :returns x: float Tensor of shape (B,T,n_embd)\n",
        "        (else)\n",
        "        :returns logits: float Tensor of shape (B, T, vocab_size)\n",
        "        :returns loss: float Tensor of shape (B) or None\n",
        "        \"\"\"\n",
        "        num_tokens = (idx != -1).type(torch.int).sum(dim=1)\n",
        "        if hidden_cache is not None:\n",
        "          num_tokens = num_tokens + hidden_cache.shape[1]\n",
        "        idx = idx.masked_fill(idx == -1, int(0)).type(torch.int)[:, :num_tokens.max().item()]\n",
        "        if targets is not None:\n",
        "          targets = targets[:, :num_tokens.max().item()]\n",
        "        attention_mask = self.get_attention_mask(num_tokens)\n",
        "        #############################################################################\n",
        "        # TODO:                                                                     #\n",
        "        # Put all the modules of a Transformer together for inference               #\n",
        "        #                                                                           #\n",
        "        # If hidden_cache exists,                                                   #\n",
        "        # then the Transformer inputs should be concatenated in the token dimension #\n",
        "        # First) All Embeddings from Hidden Cache                                   #\n",
        "        # Next)  All Embeddings of tokens from idx.                                 #\n",
        "        #                                                                           #\n",
        "        # All the modules you'll need are listed above!                              #\n",
        "        #                                                                           #\n",
        "        # Note: You can iterate through a nn.ModuleList using a standard for loop.  #\n",
        "        #                                                                           #\n",
        "        # This will take a few lines!                                               #\n",
        "        ##############################################################################\n",
        "\n",
        "        # embed words tokens into vectors\n",
        "        x = self.transformer['embedding'](idx)\n",
        "\n",
        "        # if hidden_cache has outputs from previous transformer layer\n",
        "        if hidden_cache is not None:\n",
        "          # concatenate previous transformer outputs with embeddings along first dimension\n",
        "          x = torch.cat([hidden_cache, x], dim=1)\n",
        "\n",
        "        # apply transformer blocks to embeddings\n",
        "        for block in self.transformer['h']:\n",
        "          # pass embeddings through transformer blocks\n",
        "          x = block(x, attention_mask)\n",
        "          # apply layer norm\n",
        "          x = self.transformer['ln_f'](x)\n",
        "\n",
        "        # logits vector contains score associated to words\n",
        "        # get logits from each token\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "        if return_hidden:\n",
        "            return x\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            s_logits = logits\n",
        "            if hidden_cache is not None:\n",
        "              s_logits = logits[:, hidden_cache.shape[1]-1:-1].contiguous()\n",
        "            loss = F.cross_entropy(\n",
        "                s_logits.reshape(-1, self.vocab_size), targets.reshape(-1), ignore_index=-1\n",
        "            )\n",
        "\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "#Do not change, it will break the AutoGrader\n",
        "transformer_def = In[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvahgUP5G04t",
        "outputId": "f9995f4c-21d4-4ddd-9f76-46025fd73218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 0.00M\n",
            "Success 1\n",
            "number of parameters: 0.00M\n",
            "Success 2\n",
            "number of parameters: 0.00M\n",
            "Success 3\n"
          ]
        }
      ],
      "source": [
        "#@title Test Full Transformer Forward Pass\n",
        "\n",
        "def test_transformer():\n",
        "  config = DummyTransformer.get_default_config()\n",
        "  config.vocab_size = 10\n",
        "  config.block_size = 10\n",
        "  config.n_layer = 2\n",
        "  config.n_embd = 4\n",
        "  config.n_head = 2\n",
        "  torch.set_default_device(\"cpu\")\n",
        "  set_seed(3407)\n",
        "  transformer = GenericTransformer(config)\n",
        "  idx = torch.tensor([[1, 2, 3, 4, 5, -1, -1, -1, -1, -1]], dtype=torch.long)\n",
        "  s = F.softmax(transformer(idx)[0][0, 0], dim=0)\n",
        "  assert torch.allclose(s, torch.tensor([0.1034, 0.0960, 0.1019, 0.1022, 0.1003, 0.1040, 0.0983, 0.1072, 0.0958, 0.0910], dtype=torch.float), atol=1e-5, rtol=1)\n",
        "  print(\"Success 1\")\n",
        "\n",
        "def test_transformer_loss():\n",
        "  config = DummyTransformer.get_default_config()\n",
        "  config.vocab_size = 10\n",
        "  config.block_size = 10\n",
        "  config.n_layer = 2\n",
        "  config.n_embd = 4\n",
        "  config.n_head = 2\n",
        "  torch.set_default_device(\"cpu\")\n",
        "  set_seed(3407)\n",
        "  transformer = GenericTransformer(config)\n",
        "  idx = torch.tensor([[1, 2, 3, 4, 5, -1, -1, -1, -1, -1]], dtype=torch.long)\n",
        "  target = torch.arange(5).reshape(1, 5)\n",
        "  assert torch.allclose(transformer(idx, targets=target)[1], torch.tensor(2.2973), atol=1e-4)\n",
        "  print(\"Success 2\")\n",
        "\n",
        "def test_transformer_hidden():\n",
        "  config = DummyTransformer.get_default_config()\n",
        "  config.vocab_size = 10\n",
        "  config.block_size = 10\n",
        "  config.n_layer = 2\n",
        "  config.n_embd = 4\n",
        "  config.n_head = 2\n",
        "  torch.set_default_device(\"cpu\")\n",
        "  set_seed(3407)\n",
        "  transformer = GenericTransformer(config)\n",
        "  idx = torch.tensor([[1, 2, 3, 4, 5, -1, -1, -1, -1, -1]], dtype=torch.long)\n",
        "  target = torch.arange(5).reshape(1, 5)\n",
        "  hidden = transformer(idx, targets=target, return_hidden=True)\n",
        "  assert torch.allclose(hidden[0, 0], torch.tensor([1.4417, -1.3564,  0.1549, -0.2401]), atol=1e-4)\n",
        "  print(\"Success 3\")\n",
        "\n",
        "test_transformer()\n",
        "test_transformer_loss()\n",
        "test_transformer_hidden()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYNuYWMn0iaS"
      },
      "source": [
        "#### Implement an Encoder Transformer\n",
        "\n",
        "Encoders, like the BERT model you learned about in lecture, utilize bi-directional attention. This means that in the sequence \"A B C\", the representation for token \"B\" will be influenced by tokens A *and* C. When all tokens can attend to all other tokens, the attention_mask is just a matrix of ones.\n",
        "\n",
        "However, since sentences come in a wide range of lengths, we need a way to batch sequences of different lengths together in order to maximize our GPU throughput. The most common way of doing this is called \"Padding\". When you pad an input, you add additional \"pad\" tokens to make it the same length as the longest sequence in a batch. For example, if we wanted to batch \"A B C\" and \"A B C D\" together, we would add a \"pad\" token to \"A B C\". Our resulting batch would be \\[\"A B C \\<pad\\>\", \"A B C D\"\\].\n",
        "\n",
        "Since these pad tokens are meaningless, we want to avoid having them affect our results. To do this, we remove them from the attention mask for that element in the batch. Below, you'll write a function to create such an attention mask for padded sequences given a tensor which contains the number of valid leading tokens for each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VF8grwOtG-gs"
      },
      "outputs": [],
      "source": [
        "class Encoder(GenericTransformer):\n",
        "    \"\"\"Encoder Style Transformer with Bidirectional Attention\"\"\"\n",
        "    def get_attention_mask(self, num_tokens):\n",
        "        \"\"\"\n",
        "        :param num_tokens: int Tensor of shape (batch size)\n",
        "        :returns attention_mask: int tensor of shape (batch_size, 1, max_tokens, max_tokens)\n",
        "        \"\"\"\n",
        "        B = num_tokens.shape[0]\n",
        "        max_tokens = min(self.block_size, num_tokens.max().item())\n",
        "        ##############################################################################\n",
        "        # TODO:                                                                      #\n",
        "        # Implement a padding mask function.                                         #\n",
        "        # This allows batching sequences of different lengths.                       #\n",
        "        #                                                                            #\n",
        "        # For example, for any row attention_mask[b, i] the following should be true:#\n",
        "        #               For j < num_tokens[b], attention_mask[b, i, j] = 1           #\n",
        "        #               For j >= num_tokens[b],  attention_mask[b, i, j] = 0         #\n",
        "        #                                                                            #\n",
        "        # Reference:https://huggingface.co/docs/transformers/glossary#attention-mask #                                                                #\n",
        "        #                                                                            #\n",
        "        # This should be a 1-3 line function.                                        #\n",
        "        ##############################################################################\n",
        "\n",
        "        # 1D tensor containing integers from [0, max_tokens-1]\n",
        "        # expanded to have 3D tensor shape (B, max_tokens, max_tokens)\n",
        "        attention_mask = torch.arange(max_tokens).expand(B, max_tokens, max_tokens)\n",
        "\n",
        "        # reshape num_tokens to have shape (B, 1, 1)\n",
        "        num_tokens = num_tokens.reshape(B, 1, 1)\n",
        "\n",
        "        # compare each element of attention_mask with num_tokens\n",
        "        # mark positions within the valid sequence length with 1 and padded positions 0\n",
        "        attention_mask = attention_mask < num_tokens\n",
        "\n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "        return attention_mask.reshape(B, 1, max_tokens, max_tokens)\n",
        "\n",
        "#Do not change, it will break the AutoGrader\n",
        "encoder_def = In[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXGGTqi4G-6i",
        "outputId": "95094459-2548-49ed-a6f5-8e0cb6e0cf19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 0.00M\n"
          ]
        }
      ],
      "source": [
        "#@title Test Encoder\n",
        "\n",
        "def test_encoder():\n",
        "  config = DummyTransformer.get_default_config()\n",
        "  config.vocab_size = 10\n",
        "  config.block_size = 10\n",
        "  config.n_layer = 2\n",
        "  config.n_embd = 4\n",
        "  config.n_head = 2\n",
        "  torch.set_default_device(\"cpu\")\n",
        "  set_seed(3407)\n",
        "  transformer = Encoder(config)\n",
        "  mask = transformer.get_attention_mask(torch.tensor([5, 6]))\n",
        "  assert mask[0, :, 0].sum() == 5\n",
        "  assert mask[1, :, 0].sum() == 6\n",
        "\n",
        "\n",
        "test_encoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfA1ktrT2l0a"
      },
      "source": [
        "#### Implement an Decoder Transformer\n",
        "\n",
        "Unlike Encoders, Decoders are a \"causal\" model, meaning that each prediction is only influenced by the tokens which came earlier than it in the input. While \"Encoders\" and \"Decoders\" are often discussed as different types of models, the only core difference is the attention mask used.\n",
        "\n",
        "For decoders, we want the attention mask for each token to only include the previous tokens in the sequence. Despite being functionally very different models, a \"Decoder\" can be implemented with just a one line change of the \"Encoder\" attention mask. You'll implement this below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "je6nfibPHGPr"
      },
      "outputs": [],
      "source": [
        "class Decoder(Encoder):\n",
        "    \"\"\"Decoder Style model with a Causal Attention Mask\"\"\"\n",
        "\n",
        "    def get_attention_mask(self, num_tokens):\n",
        "        \"\"\"\n",
        "        :param num_tokens: int Tensor of shape (batch size)\n",
        "        :returns attention_mask: int tensor of shape (batch_size, 1, block_size, block_size)\n",
        "        \"\"\"\n",
        "        full_attention_mask = super().get_attention_mask(num_tokens)\n",
        "        ##############################################################################\n",
        "        # TODO:                                                                      #\n",
        "        # Modify the output of the full encoder mask to create a \"causal\" mask       #\n",
        "        # such that tokens only attend to tokens which occured earlier in the input. #\n",
        "        #                                                                            #\n",
        "        # For example, for any row attention_mask[b, i} the following should be true:#\n",
        "        #               For j <= i, attention_mask[b, i, j] = 1                      #\n",
        "        #               For j > i,  attention_mask[b, i, j] = 0                      #\n",
        "        #                                                                            #\n",
        "        # This should be a one line function which modifies the full attention_mask  #\n",
        "        ##############################################################################\n",
        "\n",
        "        # create a lower triangular matrix\n",
        "        attention_mask = torch.tril(full_attention_mask)\n",
        "\n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "        return attention_mask\n",
        "\n",
        "#Do not change, it will break the AutoGrader\n",
        "decoder_def = In[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyg5zzmc_8y0",
        "outputId": "f440f26c-c8f9-4eac-f47f-902d8dff9629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 0.00M\n"
          ]
        }
      ],
      "source": [
        "#@title Test Decoder\n",
        "\n",
        "def test_decoder():\n",
        "  config = DummyTransformer.get_default_config()\n",
        "  config.vocab_size = 10\n",
        "  config.block_size = 10\n",
        "  config.n_layer = 2\n",
        "  config.n_embd = 4\n",
        "  config.n_head = 2\n",
        "  torch.set_default_device(\"cpu\")\n",
        "  set_seed(3407)\n",
        "  transformer = Decoder(config)\n",
        "  mask = transformer.get_attention_mask(torch.tensor([5, 6]))\n",
        "  for i in range(5):\n",
        "    assert mask[0, :, i].sum() == i+1\n",
        "  assert mask[0, :, 5].sum() == 5\n",
        "  for i in range(6):\n",
        "    assert mask[1, :, i].sum() == i+1\n",
        "\n",
        "test_decoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UABwUKI93c_A"
      },
      "source": [
        "#### Use your model to generate!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JEbiacH3JUF8"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, temperature=1.0):\n",
        "    \"\"\"\n",
        "    :param idx: int Tensor of shape (B, T)\n",
        "    :param max_new_tokens: int\n",
        "    :param temperature: Float\n",
        "    :returns idx: int Tensor of shape (B, T+max_new_tokens)\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO:                                                                      #\n",
        "    # Sample from your model max_new_tokens times                                #\n",
        "    # You should feed the predictions back into the model each time              #\n",
        "    #                                                                            #\n",
        "    # Adjust the probability distribution to be more or less greedy using        #\n",
        "    # the temperature parameter                                                  #\n",
        "    #                                                                            #\n",
        "    # Reference: https://huggingface.co/blog/how-to-generate#sampling            #\n",
        "    # Temperature Reference:                                                     #\n",
        "    # https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture10-nlg.pdf#page=34 #\n",
        "    ##############################################################################\n",
        "\n",
        "    # generate new tokens for current context tokens idx (B, T) s.t. we have (B, T+max_new_tokens)\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get scores tensor from model\n",
        "      logits, loss = model(idx)\n",
        "      # get last score from each sequence (to be converted to probabilties)\n",
        "      logits = logits[:, -1]\n",
        "      # apply temperature to scores\n",
        "      scaled_logits = logits / temperature\n",
        "      # get probabilities by applying softmax to scores along last dimension\n",
        "      probabilities = torch.nn.functional.softmax(scaled_logits, dim=1)\n",
        "      # sample from multivariate distribution to get next word token\n",
        "      next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "      # add next tokens to current tokens (B, T+1)\n",
        "      idx = torch.cat((idx, next_token), dim=1)\n",
        "\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return idx\n",
        "\n",
        "#Do not change, it will break the AutoGrader\n",
        "generate_def = In[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UR1Zn12LanRo"
      },
      "outputs": [],
      "source": [
        "#@title Test Generation\n",
        "\n",
        "def test_generate():\n",
        "    def dumb_model(idx):\n",
        "      l = torch.zeros(1, 1, 10)\n",
        "      l[0, 0, 0] = 100\n",
        "      l[0, 0, 5] = 90\n",
        "      return l.roll(idx[0, -1].item()+1), None\n",
        "    torch.set_default_device(\"cpu\")\n",
        "    set_seed(3047)\n",
        "    assert torch.allclose(generate(dumb_model, torch.tensor([[0]]), 6), torch.tensor([0,1,2,3,4,5,6]))\n",
        "    temp_gen_1 = generate(dumb_model, torch.tensor([[0]]), 6, temperature=10)\n",
        "    assert torch.allclose(temp_gen_1, torch.tensor([[0, 6, 2, 3, 4, 5, 6]]))\n",
        "\n",
        "test_generate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYnbvnKI3gnF"
      },
      "source": [
        "#### Implement an Encoder Decoder Transformer\n",
        "\n",
        "Now, we'll put together our Encoder and Decoder models. This combination of the two architectures allows us to maximize the signal we can draw from the input using a bi-directional encoder, while generating language using a causal decoder.\n",
        "\n",
        "Below, you'll combine your Encoder and Decoder classes in a forward function making use of the `return_hidden` and `hidden_cache` arguments we supported in our Transformer implementation to pass information between the modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "P5L7c5YifKd-"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"Encoder-Decoder Model which combines the two architectures\"\"\"\n",
        "    def __init__(self, encoder_config, decoder_config):\n",
        "        super().__init__()\n",
        "        # Add end of sequence token.\n",
        "        decoder_config.vocab_size += 1\n",
        "        self.vocab_size = decoder_config.vocab_size\n",
        "        self.encoder = Encoder(encoder_config)\n",
        "        self.decoder = Decoder(decoder_config)\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        enc_groups = self.encoder.configure_optimizers(train_config)\n",
        "        dec_groups = self.decoder.configure_optimizers(train_config)\n",
        "        return enc_groups + dec_groups\n",
        "\n",
        "    def forward(self, prefix, targets=None):\n",
        "        \"\"\"\n",
        "        :param prefix: int Tensor of shape (B,P_T)\n",
        "        :param idx: float Tensor of shape (B,P_T,n_embd)\n",
        "        :returns logits: float Tensor of shape (B, vocab_size)\n",
        "        :returns loss: float Tensor of shape (B) or None\n",
        "        \"\"\"\n",
        "        B = prefix.shape[0]\n",
        "        idx = torch.tensor([[]]).repeat(B, 1)\n",
        "        if targets is not None:\n",
        "          idx = torch.cat((idx, targets), dim=1)\n",
        "\n",
        "        ##############################################################################\n",
        "        # TODO:                                                                      #\n",
        "        # Create an Encoder Decoder Model by combining your previous transformers    #\n",
        "        # The Encoder should encode the tokens from prefix into an embeddings        #\n",
        "        # Use these in the hidden_cache to condition decoder generation              #\n",
        "        #                                                                            #\n",
        "        # This should be a 1-2 lines.                                                #\n",
        "        ##############################################################################\n",
        "\n",
        "        # encode the prefix and get the outputs from previous layer\n",
        "        hidden_states = self.encoder(prefix, return_hidden=True)\n",
        "\n",
        "        # decode outputs from previous layer to obtain scores and their loss from indicies idx\n",
        "        logits, loss = self.decoder(idx, hidden_cache=hidden_states, targets=targets)\n",
        "\n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "        return logits, loss\n",
        "\n",
        "#Do not change, it will break the AutoGrader\n",
        "encdec_def = In[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwTVJHeG4tVx"
      },
      "source": [
        "This will also require a custom `prefix_generation` function to account for the distinction between a human provided `prefix` and a model generated `idx` in the Encoder Decoder forward pass.\n",
        "\n",
        "Don't worry, this should be only a small change from your original `generate` function above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0X8dl3D64uql"
      },
      "outputs": [],
      "source": [
        "def prefix_generate(model, prefix, max_new_tokens, temperature=1.0):\n",
        "    \"\"\"\n",
        "    :param prefix: int Tensor of shape (B, T)\n",
        "    :param max_new_tokens: int\n",
        "    :param temperature: Float\n",
        "    :returns idx: int Tensor of shape (B, max_new_tokens)\n",
        "    \"\"\"\n",
        "    idx = torch.tensor([[]], dtype=torch.long)\n",
        "    ##############################################################################\n",
        "    # TODO:                                                                      #\n",
        "    # Adjust your original generation function to work Encoder-Decoder models    #\n",
        "    #                                                                            #\n",
        "    # Note: This should be a one line change from the original generate function #\n",
        "    ##############################################################################\n",
        "\n",
        "    # generate new tokens for current context tokens idx (B, T) s.t. we have (B, T+max_new_tokens)\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get scores tensor from model\n",
        "      logits, loss = model(prefix, idx)\n",
        "      # get last score from each sequence (to be converted to probabilties)\n",
        "      logits = logits[:, -1]\n",
        "      # apply temperature to scores\n",
        "      scaled_logits = logits / temperature\n",
        "      # get probabilities by applying softmax to scores along last dimension\n",
        "      probabilities = torch.nn.functional.softmax(scaled_logits, dim=1)\n",
        "      # sample from multivariate distribution to get next word token\n",
        "      next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "      # add next tokens to current tokens (B, T+1)\n",
        "      idx = torch.cat((idx, next_token), dim=1)\n",
        "\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return idx\n",
        "\n",
        "#Do not change, it will break the AutoGrader\n",
        "pref_generate_def = In[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kICaKJURf9-L",
        "outputId": "e3b264b2-28fa-4a17-9129-416ab526e33c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 0.09M\n",
            "number of parameters: 0.09M\n",
            "running on device cpu\n",
            "iter_dt 0.00ms; iter 0: train loss 1.44513\n",
            "iter_dt 62.75ms; iter 100: train loss 0.11757\n",
            "iter_dt 65.79ms; iter 200: train loss 0.05421\n",
            "iter_dt 63.06ms; iter 300: train loss 0.02787\n",
            "iter_dt 63.01ms; iter 400: train loss 0.00290\n"
          ]
        }
      ],
      "source": [
        "#@title End to End Test of Encoder Decoder Training\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from gtgpt.trainer import Trainer\n",
        "\n",
        "import pickle\n",
        "\n",
        "class SortDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for the Sort problem. E.g. for problem length 6:\n",
        "    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n",
        "    Which will feed into the transformer concatenated as:\n",
        "    input:  0 0 2 1 0 1 0 0 0 1 1\n",
        "    output: I I I I I 0 0 0 1 1 2\n",
        "    where I is \"ignore\", as the transformer is reading the input sequence\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, split, length=6, num_digits=3):\n",
        "        assert split in {'train', 'test'}\n",
        "        self.split = split\n",
        "        self.length = length\n",
        "        self.num_digits = num_digits\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000 # ...\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.num_digits\n",
        "\n",
        "    def get_block_size(self):\n",
        "        # the length of the sequence that will feed into transformer,\n",
        "        # containing concatenated input and the output, but -1 because\n",
        "        # the transformer starts making predictions at the last input element\n",
        "        return 20\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # use rejection sampling to generate an input example from the desired split\n",
        "        while True:\n",
        "            # generate some random integers\n",
        "            inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\n",
        "            # half of the time let's try to boost the number of examples that\n",
        "            # have a large number of repeats, as this is what the model seems to struggle\n",
        "            # with later in training, and they are kind of rate\n",
        "            if torch.rand(1).item() < 0.5:\n",
        "                if inp.unique().nelement() > self.length // 2:\n",
        "                    # too many unqiue digits, re-sample\n",
        "                    continue\n",
        "            # figure out if this generated example is train or test based on its hash\n",
        "            h = hash(pickle.dumps(inp.tolist()))\n",
        "            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n",
        "            if inp_split == self.split:\n",
        "                break # ok\n",
        "\n",
        "        # solve the task: i.e. sort\n",
        "        sol = torch.sort(inp)[0]\n",
        "\n",
        "        # concatenate the problem specification and the solution\n",
        "        cat = torch.cat((inp, sol), dim=0)\n",
        "\n",
        "        # the inputs to the transformer will be the offset sequence\n",
        "        x = cat[:self.length].clone()\n",
        "        y = cat[self.length:].clone()\n",
        "        # we only want to predict at output locations, mask out the loss at the input locations\n",
        "        return x, y\n",
        "\n",
        "def test_encoder_decoder():\n",
        "  # print an example instance of the dataset\n",
        "  train_dataset = SortDataset('train')\n",
        "  test_dataset = SortDataset('test')\n",
        "  x, y = train_dataset[0]\n",
        "  config = DummyTransformer.get_default_config()\n",
        "  config.vocab_size = train_dataset.get_vocab_size()\n",
        "  config.block_size = train_dataset.get_block_size()\n",
        "  config.n_layer = 3\n",
        "  config.n_embd = 48\n",
        "  config.n_head = 3\n",
        "  torch.set_default_device(\"cpu\")\n",
        "  set_seed(3407)\n",
        "  model = EncoderDecoder(config, config)\n",
        "  train_config = Trainer.get_default_config()\n",
        "  train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
        "  train_config.max_iters = 500\n",
        "  train_config.num_workers = 0\n",
        "  train_config.device = \"cpu\"\n",
        "  trainer = Trainer(train_config, model, train_dataset)\n",
        "  def batch_end_callback(trainer):\n",
        "      if trainer.iter_num % 100 == 0:\n",
        "          print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "  trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "\n",
        "  trainer.run()\n",
        "  model.eval()\n",
        "  assert torch.allclose(prefix_generate(model, torch.tensor([[2, 1, 1, 0, 1, 2]]), max_new_tokens=6), torch.tensor([[0, 1, 1, 1, 2, 2]]))\n",
        "\n",
        "test_encoder_decoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3xuBrm4449O"
      },
      "source": [
        "# You've implemented a language model!\n",
        "\n",
        "## Now let's put it to use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "6c7f3f33af484b178d16ebf08d6a43c6",
            "2cdf7aa7c3124fc7baf3f5db5c2886e0",
            "9b8fe42214494b4c829772d9172c962d",
            "f24759e3f82b4348ae000ba08a52013a",
            "d65650d920ea4eea961cd1e8816fede1",
            "259cb846d08b40c7a12dddf6ec7fdbfe",
            "15963e59a21c4c6ba194c65f4cd494d9",
            "8ea9a658c5984c7fadaffa07b277ff78",
            "665c31508f144fca8bfb5d5e251dfde1",
            "ed0f46dd913947d8a34702b4b0f89db5",
            "47164c8e77d2452fb1dcb5d0de528046",
            "2b92af1070bd401a8fdbd0daf454740e",
            "9c08d4374f7147b68ba32d84c609371c",
            "03290e45e48b4dc8b7718f53ab04fb57",
            "0932f518eab34c8c823267fbbd362652",
            "48100870d1154986a17a85f914fb6df6",
            "e5432e55d5d14db58f63e9ab6a787291",
            "6ad75d7c2e56405493f18a414ed01164",
            "d0f322b955354d438181d09985b82683",
            "ff696d5cc3854c1a84943507895ea5c2",
            "78372564fa5443ed933b64e0128fa7b1",
            "9531c9f0762e43dda2ebdb164d3f9f9f",
            "1c14c677b1204faf9676175092610d33",
            "78745d0671694f83a4cba979052d958c",
            "126c0335696c4888a2f92ffda50a36a5",
            "0d96c724e7c24fb8b2ac57f0f8bd8712",
            "8058e8d71f1f4e92a1f4a1b62badc32f",
            "190f800470294945b88248438111001b",
            "25c24a81068e4768b726481ac6f19f99",
            "bd470b8b069149849e2241f2e7ff8a37",
            "77ee48942e644b1abbfaae905945fd74",
            "1a940d49cfef4fecbe0b68cb1b9dd83b",
            "f4d0d4cf11bf4d1687c63797abec16ea",
            "123b7e303dae4461bef89de9b5ab0e07",
            "8679fa1627f343748b406688b56dace6",
            "a09f2185fce641d29569f1f961c3e1e8",
            "3ca7b790a5f846b2a54d52f16d152efc",
            "aea41f7fc597493ab9d71b422ee649f4",
            "25e70cf453244ff994d17f7eadf2e394",
            "f7fcb46e6bda4d2daaf121557d90ede7",
            "6775ad09e7e747e3ab2ad75a7c4fd3a6",
            "73b2f08bb63544aebb2fcbd7da83a652",
            "247306256f1d46c1ac1fdb3100999081",
            "071a49de62bb472a818e7bf485dcaf53",
            "d368bd5fee8549da922bc1d0155bbd4c",
            "889189d8b9c54e3f94ddb52f9a450433",
            "2fcecbfde21642fd97d1c2334913e1ab",
            "532feb8817a04627abf30b116d324180",
            "c11acdb82a1440138192a6b90bc4347a",
            "a4c7c656fb4d4ead9d0187c41228b926",
            "cbf2f457ff5f41b1839d596c60feb88d",
            "bb952396e4e548488b6515c04ac90a14",
            "d13e85014d9a417fab1ae937c4859364",
            "e7145419fa104564a861ef0153151d5b",
            "fea196d3ac4a46ee994d8dc21f3c8e6d"
          ]
        },
        "id": "W9XRRfet5CIL",
        "outputId": "a35cd8da-53c8-4521-c34a-3103802964bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.33M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c7f3f33af484b178d16ebf08d6a43c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/100k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b92af1070bd401a8fdbd0daf454740e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/13232 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c14c677b1204faf9676175092610d33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "123b7e303dae4461bef89de9b5ab0e07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/13232 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d368bd5fee8549da922bc1d0155bbd4c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Language Modeling Setup (Do Not Change)\n",
        "from gtgpt.trainer import Trainer\n",
        "from tqdm import tqdm\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.trainers import UnigramTrainer, BpeTrainer\n",
        "from tokenizers.models import Unigram, BPE\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "class LMDataset(Dataset):\n",
        "    def __init__(self, split, data, tokenizer, model):\n",
        "        assert split in {'train', 'test'}\n",
        "        self.model_type = \"EncDec\" if issubclass(type(model), EncoderDecoder) else \"Dec\"\n",
        "        if split == \"train\":\n",
        "          self.start_split = 0\n",
        "          self.end_split = 30000\n",
        "        else:\n",
        "          self.start_split = 30000\n",
        "          self.end_split = 40000\n",
        "        self.split = split\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_size = max([len(self.tokenizer.encode(inp)) for inp in self.data])\n",
        "        self.process()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[self.start_split:self.end_split])\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.tokenizer.get_vocab_size()\n",
        "\n",
        "    def get_block_size(self):\n",
        "        # the length of the sequence that will feed into transformer,\n",
        "        # containing concatenated input and the output, but -1 because\n",
        "        # the transformer starts making predictions at the last input element\n",
        "        return self.block_size\n",
        "\n",
        "    def process(self):\n",
        "      new_data = []\n",
        "      for inp in tqdm(self.data):\n",
        "        if self.model_type == \"EncDec\":\n",
        "          x_inp = inp.split(\"[SEP]\")[0] + \"[SEP]\"\n",
        "          y_inp = inp.split(\"[SEP]\")[1]\n",
        "          x = self.tokenizer.encode(x_inp)\n",
        "          y = self.tokenizer.encode(y_inp)\n",
        "        else:\n",
        "          x = self.tokenizer.encode(inp)\n",
        "          y = x[1:]\n",
        "          x = x[:-1]\n",
        "        x = x + ([-1] * (self.get_block_size() - len(x)))\n",
        "        y = y + ([-1] * (self.get_block_size() - len(y)))\n",
        "        new_data.append((x, y))\n",
        "      self.data = new_data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      x, y = self.data[self.start_split + idx]\n",
        "      return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "def format_review(row):\n",
        "  return {\"text\": f\"{row['translation']['eng']}[SEP]{row['translation']['engyay']}[END]\"}\n",
        "\n",
        "dataset = load_dataset(\"cdleong/piglatin-mt\")[\"train\"]\n",
        "data = [row[\"text\"] for row in dataset.map(format_review).to_list()]\n",
        "set_seed(3047)\n",
        "random.shuffle(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcvP7ZXLEoq0"
      },
      "source": [
        "#### Training a Language Model from Scratch\n",
        "\n",
        "Above, we set up code which loads WikiHow articles as a training dataset either for pure Decoder models or with the Title passed as a prefix for an EncoderDecoder model. We want to train a model to translate between English and Pig-Latin!\n",
        "\n",
        "Below is an implementation which achieves between 40 and 50 percent accuracy. Modify the tokenizer, architecture, or hyperparameters to  decrease the loss as much as possible and drive accuracy above 80%. Report what you changed and your intuitions for why you changed it in the report powerpoint file and upload it as a PDF to GradeScope.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SBbSkVLq5Eir"
      },
      "outputs": [],
      "source": [
        "# Incredibly Simplified Tokenizer so that you can manually hack it!\n",
        "# Feel free to add special tokens or modify as you wish.\n",
        "# For real world tokenizer usage, see https://huggingface.co/docs/tokenizers/\n",
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.DELIM = \"|[DELIM]|\"\n",
        "    self.special_tokens = [\"[SEP]\", \"[END]\"]\n",
        "    self.special_tokens = [self.stringify(list(bytes(tok, \"utf-8\"))) for tok in self.special_tokens]\n",
        "    self.vocab_size = 256 + len(self.special_tokens)\n",
        "\n",
        "  def stringify(self, b_enc):\n",
        "    s_enc = [str(b) for b in b_enc]\n",
        "    return self.DELIM.join(s_enc)\n",
        "\n",
        "  def get_vocab_size(self):\n",
        "    return self.vocab_size\n",
        "\n",
        "  def encode(self, inp):\n",
        "    s_enc = self.stringify(list(bytes(inp, \"utf-8\")))\n",
        "    for i, tok in enumerate(self.special_tokens):\n",
        "      s_enc = s_enc.replace(tok, str(255+i+1))\n",
        "    return [int(s) for s in s_enc.split(self.DELIM)]\n",
        "\n",
        "  def decode(self, inp):\n",
        "    s_enc = self.stringify(inp)\n",
        "    for i, tok in enumerate(self.special_tokens):\n",
        "      s_enc = s_enc.replace(str(255+i+1), tok)\n",
        "    return  bytes([int(c) for c in s_enc.split(self.DELIM)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5cfrrAkfDIQT"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.set_default_device(DEVICE)\n",
        "def train(data, model_type=\"Decoder\",\n",
        "          learning_rate = 5e-4,\n",
        "          batch_size = 16,\n",
        "          max_iters = 10000,\n",
        "          dec_n_layer=1,\n",
        "          dec_n_embd=52,\n",
        "          dec_n_head = 1,\n",
        "          enc_n_layer=None,\n",
        "          enc_n_embd=None,\n",
        "          enc_n_head=None):\n",
        "  # Model Setup\n",
        "  tokenizer = Tokenizer()\n",
        "  dec_config = DummyTransformer.get_default_config()\n",
        "  dec_config.vocab_size = tokenizer.get_vocab_size()\n",
        "  dec_config.block_size = max([len(tokenizer.encode(inp)) for inp in data])\n",
        "  dec_config.n_layer = dec_n_layer\n",
        "  dec_config.n_embd = dec_n_embd\n",
        "  dec_config.n_head = dec_n_head\n",
        "  if model_type == \"Decoder\":\n",
        "    model = Decoder(dec_config)\n",
        "  else:\n",
        "    enc_config = DummyTransformer.get_default_config()\n",
        "    enc_config.vocab_size = tokenizer.get_vocab_size()\n",
        "    enc_config.block_size = max([len(tokenizer.encode(inp)) for inp in data])\n",
        "    enc_config.n_layer = enc_n_layer\n",
        "    enc_config.n_embd = enc_n_embd\n",
        "    enc_config.n_head = enc_n_head\n",
        "    model = EncoderDecoder(enc_config, dec_config)\n",
        "\n",
        "  # Training Config\n",
        "  train_config = Trainer.get_default_config()\n",
        "  train_config.learning_rate = learning_rate\n",
        "  train_config.max_iters = max_iters\n",
        "  train_config.batch_size = batch_size\n",
        "  train_config.num_workers = 0\n",
        "  train_config.device = DEVICE\n",
        "  train_ds = LMDataset(\"train\", data, tokenizer, model)\n",
        "  # Training Loop\n",
        "  trainer = Trainer(train_config, model, train_ds)\n",
        "  def batch_end_callback(trainer):\n",
        "      if trainer.iter_num % 100 == 0:\n",
        "          tqdm.write(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "          prefix = torch.tensor([tokenizer.encode(\"translate this to piglatin[SEP]\")])\n",
        "          if model_type == \"Decoder\":\n",
        "            output = generate(model, prefix, 100, 0.1)\n",
        "          else:\n",
        "            output = prefix_generate(model, prefix, 100, 0.1)\n",
        "          print(tokenizer.decode(output.cpu().numpy()[0]).split(bytes(\"[END]\", \"utf-8\"))[0])\n",
        "  trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "  trainer.run()\n",
        "  return model, trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B7WzG16__KFU",
        "outputId": "407f5325-fe1a-416f-806e-1db44d3ef25a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 1.25M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13232/13232 [00:01<00:00, 11813.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on device cuda:0\n",
            "iter_dt 0.00ms; iter 0: train loss 5.58198\n",
            "b\"translate this to piglatin[SEP]8!R\\x81y\\xa7\\xa5\\xc3\\x01\\xa3\\xef/7->n *\\x9fd\\xccg\\xaa\\xbb \\x8e\\x1c5sZ\\xcd\\xfd\\x03\\x1e\\xd6)\\xb2\\xc5R[\\x03ma!\\x11\\xe6\\xf6r\\x02\\x80%\\x89@\\xaaa|t\\x83'\\xdab\\x16\\x90\\xa1\\xfe\\xbe?\\xe4#\\xc3\\xe2\\xc5\\x93p\\xb5\\xd5\\xf6\\xdcD{\\x8aO\\x94\\r\\xec\\xc5w7\\xa3\\xa8\\xaa\\xa5\\x9c\\x9b\\xde ay \\xd6\"\n",
            "iter_dt 74.20ms; iter 100: train loss 2.27068\n",
            "b'translate this to piglatin[SEP]e the the the the the the the the the the ore-thay ay e-ay e-ay ay ay e-ay e-thay ay e-ay e-ay ay e-'\n",
            "iter_dt 78.56ms; iter 200: train loss 2.14884\n",
            "b'translate this to piglatin[SEP]ended ay orederede the t thand ore therere[SEP]e-thay e-ay e-ay ay ay e-ay e-thay e-ay e-thay ay ay e-ay'\n",
            "iter_dt 87.47ms; iter 300: train loss 1.96287\n",
            "b'translate this to piglatin[SEP]ous-ay ofrere-ay oreray theste-thay erand-thay of-ay e-ay e-ay ond-thay e-thay e-thay ond-thay e-tha'\n",
            "iter_dt 77.29ms; iter 400: train loss 1.92862\n",
            "b'translate this to piglatin[SEP]es-thay ont-thay onand-thay o o ong onth-thay e-thay e-thay e-thay e-thay e-thay e-thay e-thay e-tha'\n",
            "iter_dt 75.33ms; iter 500: train loss 1.86225\n",
            "b'translate this to piglatin[SEP]ent-tay e-thay or-thay of-thay ay of-thath-thay o-thay o-ay e-thay ay o-th-thay o-ay e-thay e-thay o'\n",
            "iter_dt 71.97ms; iter 600: train loss 1.86478\n",
            "b'translate this to piglatin[SEP]erer-ay engrerere-ay or-tay ongenge ers-ay'\n",
            "iter_dt 74.32ms; iter 700: train loss 1.84802\n",
            "b'translate this to piglatin[SEP]ent-tay ant-tay e-thay e-thay of-t-thay'\n",
            "iter_dt 76.20ms; iter 800: train loss 1.79833\n",
            "b'translate this to piglatin[SEP]athintin-ay arint-ay of-ay orth-ay ous-t-thay'\n",
            "iter_dt 74.62ms; iter 900: train loss 1.70282\n",
            "b'translate this to piglatin[SEP]enttintitiy-ay'\n",
            "iter_dt 74.63ms; iter 1000: train loss 1.73421\n",
            "b'translate this to piglatin[SEP]arstiner-ay and-ay are-thay and-ay they they[SEP]at-ay ant-ay at-ay at-thay at-tay art-bay arte-bay ad-t'\n",
            "iter_dt 80.30ms; iter 1100: train loss 1.67411\n",
            "b'translate this to piglatin[SEP]ansssay-tay atss-ay eples-thay'\n",
            "iter_dt 78.82ms; iter 1200: train loss 1.58103\n",
            "b'translate this to piglatin[SEP]ansst-ay es-tay ast-tay ancary-ay'\n",
            "iter_dt 76.59ms; iter 1300: train loss 1.47019\n",
            "b'translate this to piglatin[SEP]asprs-tay is-thay isintion-ay'\n",
            "iter_dt 74.47ms; iter 1400: train loss 1.35797\n",
            "b'translate this to piglatin[SEP]ansans-tay isatis-ay im-fay imping-ay'\n",
            "iter_dt 74.95ms; iter 1500: train loss 1.25329\n",
            "b'translate this to piglatin[SEP]and-thay is-thay icaing-ay it-thay otin-ay'\n",
            "iter_dt 79.53ms; iter 1600: train loss 1.16147\n",
            "b'translate this to piglatin[SEP]anndsay-tay is-thay igh-ay igating-pay'\n",
            "iter_dt 75.24ms; iter 1700: train loss 1.06965\n",
            "b'translate this to piglatin[SEP]anshat-tay is-tay o-tay iglatin-ay itunt-ay itinn-ay is-blay itinin-ay'\n",
            "iter_dt 74.52ms; iter 1800: train loss 1.05835\n",
            "b'translate this to piglatin[SEP]andand-tray ish-tay o-ay igatin-pay'\n",
            "iter_dt 75.38ms; iter 1900: train loss 1.02209\n",
            "b'translate this to piglatin[SEP]antand-tray is-thay o-tay iglatin-pay ito-fay igatinin-ay'\n",
            "iter_dt 91.58ms; iter 2000: train loss 0.96585\n",
            "b'translate this to piglatin[SEP]anslate-tray is-tay iglatinin-pay'\n",
            "iter_dt 75.03ms; iter 2100: train loss 0.91476\n",
            "b'translate this to piglatin[SEP]anls-tray o-thay is-tay opatinin-pay'\n",
            "iter_dt 76.09ms; iter 2200: train loss 0.87315\n",
            "b'translate this to piglatin[SEP]anslats-tray is-tay iglatin-pay ig-paginin-ay itlionin-pay'\n",
            "iter_dt 74.62ms; iter 2300: train loss 0.86634\n",
            "b'translate this to piglatin[SEP]anslan-tray is-thay oig-tay iglatin-ay otin-pay'\n",
            "iter_dt 76.93ms; iter 2400: train loss 0.88694\n",
            "b'translate this to piglatin[SEP]ananss-tray is-thay iglin-pay itinin-ay'\n",
            "iter_dt 72.66ms; iter 2500: train loss 0.78454\n",
            "b'translate this to piglatin[SEP]anslatl-tray o-tay o-tay igatin-pay inatin-ay'\n",
            "iter_dt 75.85ms; iter 2600: train loss 0.83364\n",
            "b'translate this to piglatin[SEP]anslans-thray o-thay iglatin-pay igatin-pay igin-pay'\n",
            "iter_dt 72.55ms; iter 2700: train loss 0.81394\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay iglatin-ay is-pay'\n",
            "iter_dt 78.44ms; iter 2800: train loss 0.77108\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay iga-pay'\n",
            "iter_dt 76.43ms; iter 2900: train loss 0.78872\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay iglatin-pay'\n",
            "iter_dt 76.36ms; iter 3000: train loss 0.78979\n",
            "b'translate this to piglatin[SEP]anslanlate-tlay is-tay o-tay iglatin-pay igatinninn-ay'\n",
            "iter_dt 75.09ms; iter 3100: train loss 0.70688\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay igla-tay igatin-pay'\n",
            "iter_dt 75.76ms; iter 3200: train loss 0.77012\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 86.02ms; iter 3300: train loss 0.75210\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay iglating-tay'\n",
            "iter_dt 75.68ms; iter 3400: train loss 0.70459\n",
            "b'translate this to piglatin[SEP]anslatlse-tray is-tay o-tay iglatin-pay'\n",
            "iter_dt 74.75ms; iter 3500: train loss 0.77723\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay igatinn-pay'\n",
            "iter_dt 72.16ms; iter 3600: train loss 0.72564\n",
            "b'translate this to piglatin[SEP]anslatlate-tray is-thay o-pay iglatina-pay igatin-pay'\n",
            "iter_dt 87.88ms; iter 3700: train loss 0.72455\n",
            "b'translate this to piglatin[SEP]anslatle-tray is-thay o-tay iglatini-pay igni-play'\n",
            "iter_dt 75.30ms; iter 3800: train loss 0.70111\n",
            "b'translate this to piglatin[SEP]anlate-tray is-thay o-tay iglatinnin-pay'\n",
            "iter_dt 74.74ms; iter 3900: train loss 0.72636\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay iglat-tay iglatinin-pay'\n",
            "iter_dt 74.70ms; iter 4000: train loss 0.65835\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-thay o-pay'\n",
            "iter_dt 81.61ms; iter 4100: train loss 0.64268\n",
            "b'translate this to piglatin[SEP]anslatesl-tray is-tay iglatin-pay'\n",
            "iter_dt 76.56ms; iter 4200: train loss 0.64399\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.08ms; iter 4300: train loss 0.64497\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatina-pay otin-pay'\n",
            "iter_dt 75.14ms; iter 4400: train loss 0.65235\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 76.67ms; iter 4500: train loss 0.61278\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n",
            "iter_dt 76.54ms; iter 4600: train loss 0.63404\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.77ms; iter 4700: train loss 0.64781\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.71ms; iter 4800: train loss 0.61641\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 72.59ms; iter 4900: train loss 0.60011\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 82.17ms; iter 5000: train loss 0.60256\n",
            "b'translate this to piglatin[SEP]anslate-tray is-tay o-tay igllatin-pay'\n",
            "iter_dt 76.50ms; iter 5100: train loss 0.59047\n",
            "b'translate this to piglatin[SEP]anslate-tray is-tay o-tay iglatin-pay iglatin-pay'\n",
            "iter_dt 75.99ms; iter 5200: train loss 0.62568\n",
            "b'translate this to piglatin[SEP]answate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.76ms; iter 5300: train loss 0.64992\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-play'\n",
            "iter_dt 84.71ms; iter 5400: train loss 0.66524\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n",
            "iter_dt 81.27ms; iter 5500: train loss 0.57917\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 72.72ms; iter 5600: train loss 0.61561\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n",
            "iter_dt 76.82ms; iter 5700: train loss 0.55698\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 102.97ms; iter 5800: train loss 0.54903\n",
            "b'translate this to piglatin[SEP]anslate-thray is-tay o-tay igatlatin-pay'\n",
            "iter_dt 76.99ms; iter 5900: train loss 0.59956\n",
            "b'translate this to piglatin[SEP]anslate-tray is-tay o-tay iglatin-pay'\n",
            "iter_dt 75.85ms; iter 6000: train loss 0.61373\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-pay iglatin-pay'\n",
            "iter_dt 74.69ms; iter 6100: train loss 0.54319\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinatin-pay'\n",
            "iter_dt 76.63ms; iter 6200: train loss 0.58666\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 75.86ms; iter 6300: train loss 0.55928\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 79.12ms; iter 6400: train loss 0.59316\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.19ms; iter 6500: train loss 0.55271\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 74.60ms; iter 6600: train loss 0.53050\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatina-play'\n",
            "iter_dt 81.42ms; iter 6700: train loss 0.57289\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay iglatin-pay'\n",
            "iter_dt 78.60ms; iter 6800: train loss 0.56199\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 78.00ms; iter 6900: train loss 0.56130\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n",
            "iter_dt 80.29ms; iter 7000: train loss 0.56597\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 85.35ms; iter 7100: train loss 0.57151\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 80.09ms; iter 7200: train loss 0.55271\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 78.45ms; iter 7300: train loss 0.59161\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatina-pay'\n",
            "iter_dt 76.06ms; iter 7400: train loss 0.61015\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 78.85ms; iter 7500: train loss 0.56647\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-play'\n",
            "iter_dt 76.77ms; iter 7600: train loss 0.54574\n",
            "b'translate this to piglatin[SEP]anslate-tray is-tay iglatin-pay'\n",
            "iter_dt 79.51ms; iter 7700: train loss 0.54031\n",
            "b'translate this to piglatin[SEP]anslate-tray is-tay o-tay iglatin-pay'\n",
            "iter_dt 72.99ms; iter 7800: train loss 0.55066\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay iglatin-pay'\n",
            "iter_dt 75.11ms; iter 7900: train loss 0.53344\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay iglan-pay'\n",
            "iter_dt 84.42ms; iter 8000: train loss 0.55286\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.05ms; iter 8100: train loss 0.53760\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.12ms; iter 8200: train loss 0.52359\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.79ms; iter 8300: train loss 0.50774\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 83.59ms; iter 8400: train loss 0.52457\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.21ms; iter 8500: train loss 0.50608\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.18ms; iter 8600: train loss 0.52956\n",
            "b'translate this to piglatin[SEP]anslate-tray is-tay o-tay iglatin-pay'\n",
            "iter_dt 72.55ms; iter 8700: train loss 0.50323\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 87.20ms; iter 8800: train loss 0.47878\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.45ms; iter 8900: train loss 0.54299\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.81ms; iter 9000: train loss 0.50621\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.28ms; iter 9100: train loss 0.50325\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 74.10ms; iter 9200: train loss 0.49983\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay oglatinin-pay'\n",
            "iter_dt 77.64ms; iter 9300: train loss 0.49924\n",
            "b'translate this to piglatin[SEP]anslate-tray is-tay o-tay iglatin-pay'\n",
            "iter_dt 74.46ms; iter 9400: train loss 0.50318\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 74.64ms; iter 9500: train loss 0.50366\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.94ms; iter 9600: train loss 0.53598\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 81.10ms; iter 9700: train loss 0.50449\n",
            "b'translate this to piglatin[SEP]anslatlate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 73.49ms; iter 9800: train loss 0.52580\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.63ms; iter 9900: train loss 0.47186\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.46ms; iter 10000: train loss 0.49452\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay o-tay iglatin-pay'\n",
            "iter_dt 88.48ms; iter 10100: train loss 0.50509\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 74.42ms; iter 10200: train loss 0.51396\n",
            "b'translate this to piglatin[SEP]anslate-tray is-tay o-pay iglatin-pay'\n",
            "iter_dt 76.08ms; iter 10300: train loss 0.50415\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin'\n",
            "iter_dt 76.93ms; iter 10400: train loss 0.49024\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 83.85ms; iter 10500: train loss 0.48817\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.25ms; iter 10600: train loss 0.49769\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.90ms; iter 10700: train loss 0.49049\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 73.78ms; iter 10800: train loss 0.50507\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.70ms; iter 10900: train loss 0.48158\n",
            "b'translate this to piglatin[SEP]anslate-tray is-tay o-tay iglatin-pay'\n",
            "iter_dt 76.46ms; iter 11000: train loss 0.49312\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.72ms; iter 11100: train loss 0.48118\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 84.12ms; iter 11200: train loss 0.45389\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin'\n",
            "iter_dt 77.82ms; iter 11300: train loss 0.45963\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatina-pay'\n",
            "iter_dt 75.27ms; iter 11400: train loss 0.44883\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 78.74ms; iter 11500: train loss 0.48004\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.98ms; iter 11600: train loss 0.46641\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.43ms; iter 11700: train loss 0.45889\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 80.23ms; iter 11800: train loss 0.47976\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay iglatin-pay'\n",
            "iter_dt 76.66ms; iter 11900: train loss 0.47638\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.00ms; iter 12000: train loss 0.46224\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 74.93ms; iter 12100: train loss 0.50372\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 103.95ms; iter 12200: train loss 0.52576\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 72.94ms; iter 12300: train loss 0.47841\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.78ms; iter 12400: train loss 0.49173\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.84ms; iter 12500: train loss 0.47004\n",
            "b'translate this to piglatin[SEP]ansrate-tray is-thay o-tay iglay-pay'\n",
            "iter_dt 78.97ms; iter 12600: train loss 0.46135\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 79.54ms; iter 12700: train loss 0.48113\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.57ms; iter 12800: train loss 0.42620\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.49ms; iter 12900: train loss 0.46525\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.26ms; iter 13000: train loss 0.49931\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 79.69ms; iter 13100: train loss 0.49785\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.69ms; iter 13200: train loss 0.47499\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.49ms; iter 13300: train loss 0.45875\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 80.95ms; iter 13400: train loss 0.47026\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 88.34ms; iter 13500: train loss 0.48588\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 78.16ms; iter 13600: train loss 0.47141\n",
            "b'translate this to piglatin[SEP]anslate-tray is-tay o-tay iglatin-pay'\n",
            "iter_dt 75.63ms; iter 13700: train loss 0.43907\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.74ms; iter 13800: train loss 0.49503\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.42ms; iter 13900: train loss 0.44641\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.78ms; iter 14000: train loss 0.48786\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.95ms; iter 14100: train loss 0.47110\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinatin-pay'\n",
            "iter_dt 74.75ms; iter 14200: train loss 0.44032\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.26ms; iter 14300: train loss 0.45932\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin'\n",
            "iter_dt 76.35ms; iter 14400: train loss 0.48333\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.45ms; iter 14500: train loss 0.43003\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 74.55ms; iter 14600: train loss 0.46974\n",
            "b'translate this to piglatin[SEP]anslate-tray is-th-thay o-tay iglatin-pay'\n",
            "iter_dt 79.23ms; iter 14700: train loss 0.45824\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 83.04ms; iter 14800: train loss 0.42165\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin'\n",
            "iter_dt 77.21ms; iter 14900: train loss 0.42540\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.56ms; iter 15000: train loss 0.43711\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 74.97ms; iter 15100: train loss 0.44361\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 84.26ms; iter 15200: train loss 0.46036\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.56ms; iter 15300: train loss 0.42680\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-pay o-pay iglatin-pay'\n",
            "iter_dt 75.06ms; iter 15400: train loss 0.48666\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 83.23ms; iter 15500: train loss 0.45840\n",
            "b'translate this to piglatin[SEP]ansllate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 72.54ms; iter 15600: train loss 0.42628\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.89ms; iter 15700: train loss 0.46214\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.88ms; iter 15800: train loss 0.47820\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.27ms; iter 15900: train loss 0.45190\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.31ms; iter 16000: train loss 0.43748\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 93.63ms; iter 16100: train loss 0.43658\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.11ms; iter 16200: train loss 0.43614\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.41ms; iter 16300: train loss 0.47671\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 78.84ms; iter 16400: train loss 0.44601\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 98.32ms; iter 16500: train loss 0.41589\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 74.82ms; iter 16600: train loss 0.47065\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.03ms; iter 16700: train loss 0.45930\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.81ms; iter 16800: train loss 0.43886\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 100.87ms; iter 16900: train loss 0.44895\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.37ms; iter 17000: train loss 0.45959\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.09ms; iter 17100: train loss 0.46164\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.58ms; iter 17200: train loss 0.45729\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatingl-pay'\n",
            "iter_dt 77.01ms; iter 17300: train loss 0.41309\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.10ms; iter 17400: train loss 0.45722\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.02ms; iter 17500: train loss 0.43585\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.32ms; iter 17600: train loss 0.43248\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.68ms; iter 17700: train loss 0.45363\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 84.19ms; iter 17800: train loss 0.45193\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.78ms; iter 17900: train loss 0.43961\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.17ms; iter 18000: train loss 0.43810\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 75.01ms; iter 18100: train loss 0.44276\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 94.67ms; iter 18200: train loss 0.44407\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 72.90ms; iter 18300: train loss 0.43734\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 80.37ms; iter 18400: train loss 0.44262\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-thay iglain-pay'\n",
            "iter_dt 75.94ms; iter 18500: train loss 0.43330\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 78.11ms; iter 18600: train loss 0.44050\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.44ms; iter 18700: train loss 0.41699\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-pay iglatin-pay'\n",
            "iter_dt 75.98ms; iter 18800: train loss 0.41500\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 75.28ms; iter 18900: train loss 0.43933\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.22ms; iter 19000: train loss 0.45520\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 74.50ms; iter 19100: train loss 0.43710\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.14ms; iter 19200: train loss 0.42689\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.15ms; iter 19300: train loss 0.41969\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.53ms; iter 19400: train loss 0.43308\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 103.71ms; iter 19500: train loss 0.43245\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.27ms; iter 19600: train loss 0.42005\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.08ms; iter 19700: train loss 0.43057\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.72ms; iter 19800: train loss 0.42695\n",
            "b'translate this to piglatin[SEP]anslate-tray o-thay iglating-pay'\n",
            "iter_dt 86.97ms; iter 19900: train loss 0.45853\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.18ms; iter 20000: train loss 0.42153\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 74.77ms; iter 20100: train loss 0.45524\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.07ms; iter 20200: train loss 0.39897\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.59ms; iter 20300: train loss 0.42232\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.25ms; iter 20400: train loss 0.42702\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.57ms; iter 20500: train loss 0.42398\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 78.38ms; iter 20600: train loss 0.42633\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 47.91ms; iter 20700: train loss 0.44631\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 104.88ms; iter 20800: train loss 0.41914\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 80.28ms; iter 20900: train loss 0.42592\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.31ms; iter 21000: train loss 0.43545\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.96ms; iter 21100: train loss 0.42718\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 90.99ms; iter 21200: train loss 0.40058\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 74.93ms; iter 21300: train loss 0.43807\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-thay iglatingin-pay'\n",
            "iter_dt 76.16ms; iter 21400: train loss 0.43005\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.91ms; iter 21500: train loss 0.41968\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay is-tay o-tay iglatin-pay'\n",
            "iter_dt 76.22ms; iter 21600: train loss 0.39806\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.47ms; iter 21700: train loss 0.41883\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 80.19ms; iter 21800: train loss 0.43148\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay is-tay o-pay iglatin-pay'\n",
            "iter_dt 79.30ms; iter 21900: train loss 0.42772\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.31ms; iter 22000: train loss 0.40409\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 89.07ms; iter 22100: train loss 0.43410\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 77.41ms; iter 22200: train loss 0.40990\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 73.20ms; iter 22300: train loss 0.40973\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 74.66ms; iter 22400: train loss 0.41881\n",
            "b'translate this to piglatin[SEP]anslanslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 91.13ms; iter 22500: train loss 0.39990\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 76.06ms; iter 22600: train loss 0.42925\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatini-pay'\n",
            "iter_dt 76.22ms; iter 22700: train loss 0.41672\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.60ms; iter 22800: train loss 0.43741\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinatin-pay'\n",
            "iter_dt 74.63ms; iter 22900: train loss 0.41842\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-play'\n",
            "iter_dt 76.81ms; iter 23000: train loss 0.41799\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-pay iglatin-pay'\n",
            "iter_dt 76.63ms; iter 23100: train loss 0.41266\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 78.71ms; iter 23200: train loss 0.38416\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.95ms; iter 23300: train loss 0.42130\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinn-pay'\n",
            "iter_dt 88.35ms; iter 23400: train loss 0.41082\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.43ms; iter 23500: train loss 0.40141\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.55ms; iter 23600: train loss 0.41592\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 75.74ms; iter 23700: train loss 0.40462\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 79.21ms; iter 23800: train loss 0.40449\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.22ms; iter 23900: train loss 0.44863\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.34ms; iter 24000: train loss 0.43447\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay iglating-tay'\n",
            "iter_dt 76.17ms; iter 24100: train loss 0.41580\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 89.78ms; iter 24200: train loss 0.42201\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 72.50ms; iter 24300: train loss 0.41545\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.34ms; iter 24400: train loss 0.41281\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 79.04ms; iter 24500: train loss 0.41597\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay igllating-pay'\n",
            "iter_dt 78.96ms; iter 24600: train loss 0.41649\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n",
            "iter_dt 74.78ms; iter 24700: train loss 0.42364\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 77.10ms; iter 24800: train loss 0.40631\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n",
            "iter_dt 76.70ms; iter 24900: train loss 0.39037\n",
            "b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmodel, trainer = train(data, model_type=\"Decoder\",\\n          learning_rate = 5e-4,\\n          batch_size = 16,\\n          max_iters = 10000,\\n          dec_n_layer=4,\\n          dec_n_embd=64,\\n          dec_n_head =2,\\n          enc_n_layer=None,\\n          enc_n_embd=None,\\n          enc_n_head=None)\\nmodel.eval()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "model, trainer = train(data, model_type=\"Decoder\",\n",
        "          learning_rate = 3e-4,\n",
        "          batch_size = 32,\n",
        "          max_iters = 25000,\n",
        "          dec_n_layer=6,\n",
        "          dec_n_embd=128,\n",
        "          dec_n_head =8,\n",
        "          enc_n_layer=None,\n",
        "          enc_n_embd=None,\n",
        "          enc_n_head=None)\n",
        "model.eval()\n",
        "\n",
        "'''\n",
        "model, trainer = train(data, model_type=\"Decoder\",\n",
        "          learning_rate = 5e-4,\n",
        "          batch_size = 16,\n",
        "          max_iters = 10000,\n",
        "          dec_n_layer=4,\n",
        "          dec_n_embd=64,\n",
        "          dec_n_head =2,\n",
        "          enc_n_layer=None,\n",
        "          enc_n_embd=None,\n",
        "          enc_n_head=None)\n",
        "model.eval()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA77eMgfCsgA",
        "outputId": "c38840c4-f7fe-476f-efd3-cadae4eb9fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [03:13<00:00,  1.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Exact Match: 93/100 = 93.00% correct\n",
            "BLEU = 100.00 100.0/100.0/100.0/100.0 (BP = 1.000 ratio = 1.000 hyp_len = 13 ref_len = 13)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sacrebleu.metrics import BLEU\n",
        "\n",
        "def eval(trainer, data, tokenizer):\n",
        "    bleu = BLEU()\n",
        "    results = []\n",
        "    mistakes_printed_already = 0\n",
        "    tgts = []\n",
        "    cands = []\n",
        "    for sent in tqdm(data[10000:10100]):\n",
        "        inp = torch.tensor([tokenizer.encode(sent.split(\"[SEP]\")[0] + \"[SEP]\")])\n",
        "        tgt = bytes(sent.split(\"[SEP]\")[1].split(\"[END]\")[0], \"utf-8\")\n",
        "        cat = generate(model, inp, model.block_size-len(inp[0]), 0.1)\n",
        "        tgt_candidate = tokenizer.decode(cat.cpu().numpy()[0])\n",
        "        tgt_candidate = tgt_candidate.split(b\"[END]\")[0].split(b\"[SEP]\")[1]\n",
        "        # compare the predicted sequence to the true sequence\n",
        "        tgts.append([str(tgt)])\n",
        "        cands.append(str(tgt_candidate))\n",
        "        correct = (tgt == tgt_candidate)\n",
        "        results.append(correct)\n",
        "    results = torch.tensor(results).type(torch.float)\n",
        "    print(\"\\n\\nExact Match: %d/%d = %.2f%% correct\" % (torch.sum(results), len(results), 100*torch.mean(results)))\n",
        "    score = bleu.corpus_score(cands, tgts)\n",
        "    print(score)\n",
        "\n",
        "    return results\n",
        "\n",
        "with torch.no_grad():\n",
        "  results = eval(trainer, data, Tokenizer())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1lWwrT4r3HGF",
        "outputId": "219b2b58-70a8-4f99-fef7-7e3d324a3555"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_3e01e317-21f5-4070-872e-0e4f367bbf9e\", \"my_llm_implementation.py\", 15876)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Assignment Export - Upload the `my_llm_implementation.py` file to GradeScope\n",
        "\n",
        "with open(\"./my_llm_implementation.py\", \"w\") as f:\n",
        "  f.write(setup_block.split(\"#Do not change, it will break the AutoGrader\")[0])\n",
        "  f.write(embedding_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n",
        "  f.write(mha_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n",
        "  f.write(block_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n",
        "  f.write(transformer_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n",
        "  f.write(encoder_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n",
        "  f.write(decoder_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n",
        "  f.write(generate_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n",
        "  f.write(encdec_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n",
        "  f.write(pref_generate_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n",
        "\n",
        "# If you decide to do this assignment not on Colab, you'll need to simply find the file\n",
        "from google.colab import files\n",
        "files.download('./my_llm_implementation.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI5J9d5uMGgO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c7f3f33af484b178d16ebf08d6a43c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cdf7aa7c3124fc7baf3f5db5c2886e0",
              "IPY_MODEL_9b8fe42214494b4c829772d9172c962d",
              "IPY_MODEL_f24759e3f82b4348ae000ba08a52013a"
            ],
            "layout": "IPY_MODEL_d65650d920ea4eea961cd1e8816fede1"
          }
        },
        "2cdf7aa7c3124fc7baf3f5db5c2886e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_259cb846d08b40c7a12dddf6ec7fdbfe",
            "placeholder": "​",
            "style": "IPY_MODEL_15963e59a21c4c6ba194c65f4cd494d9",
            "value": "Downloading data: 100%"
          }
        },
        "9b8fe42214494b4c829772d9172c962d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ea9a658c5984c7fadaffa07b277ff78",
            "max": 1328408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_665c31508f144fca8bfb5d5e251dfde1",
            "value": 1328408
          }
        },
        "f24759e3f82b4348ae000ba08a52013a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed0f46dd913947d8a34702b4b0f89db5",
            "placeholder": "​",
            "style": "IPY_MODEL_47164c8e77d2452fb1dcb5d0de528046",
            "value": " 1.33M/1.33M [00:02&lt;00:00, 523kB/s]"
          }
        },
        "d65650d920ea4eea961cd1e8816fede1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "259cb846d08b40c7a12dddf6ec7fdbfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15963e59a21c4c6ba194c65f4cd494d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ea9a658c5984c7fadaffa07b277ff78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "665c31508f144fca8bfb5d5e251dfde1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed0f46dd913947d8a34702b4b0f89db5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47164c8e77d2452fb1dcb5d0de528046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b92af1070bd401a8fdbd0daf454740e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c08d4374f7147b68ba32d84c609371c",
              "IPY_MODEL_03290e45e48b4dc8b7718f53ab04fb57",
              "IPY_MODEL_0932f518eab34c8c823267fbbd362652"
            ],
            "layout": "IPY_MODEL_48100870d1154986a17a85f914fb6df6"
          }
        },
        "9c08d4374f7147b68ba32d84c609371c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5432e55d5d14db58f63e9ab6a787291",
            "placeholder": "​",
            "style": "IPY_MODEL_6ad75d7c2e56405493f18a414ed01164",
            "value": "Downloading data: 100%"
          }
        },
        "03290e45e48b4dc8b7718f53ab04fb57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0f322b955354d438181d09985b82683",
            "max": 100285,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff696d5cc3854c1a84943507895ea5c2",
            "value": 100285
          }
        },
        "0932f518eab34c8c823267fbbd362652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78372564fa5443ed933b64e0128fa7b1",
            "placeholder": "​",
            "style": "IPY_MODEL_9531c9f0762e43dda2ebdb164d3f9f9f",
            "value": " 100k/100k [00:00&lt;00:00, 394kB/s]"
          }
        },
        "48100870d1154986a17a85f914fb6df6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5432e55d5d14db58f63e9ab6a787291": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad75d7c2e56405493f18a414ed01164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0f322b955354d438181d09985b82683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff696d5cc3854c1a84943507895ea5c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78372564fa5443ed933b64e0128fa7b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9531c9f0762e43dda2ebdb164d3f9f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c14c677b1204faf9676175092610d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78745d0671694f83a4cba979052d958c",
              "IPY_MODEL_126c0335696c4888a2f92ffda50a36a5",
              "IPY_MODEL_0d96c724e7c24fb8b2ac57f0f8bd8712"
            ],
            "layout": "IPY_MODEL_8058e8d71f1f4e92a1f4a1b62badc32f"
          }
        },
        "78745d0671694f83a4cba979052d958c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_190f800470294945b88248438111001b",
            "placeholder": "​",
            "style": "IPY_MODEL_25c24a81068e4768b726481ac6f19f99",
            "value": "Generating train split: 100%"
          }
        },
        "126c0335696c4888a2f92ffda50a36a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd470b8b069149849e2241f2e7ff8a37",
            "max": 13232,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77ee48942e644b1abbfaae905945fd74",
            "value": 13232
          }
        },
        "0d96c724e7c24fb8b2ac57f0f8bd8712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a940d49cfef4fecbe0b68cb1b9dd83b",
            "placeholder": "​",
            "style": "IPY_MODEL_f4d0d4cf11bf4d1687c63797abec16ea",
            "value": " 13232/13232 [00:00&lt;00:00, 287403.38 examples/s]"
          }
        },
        "8058e8d71f1f4e92a1f4a1b62badc32f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "190f800470294945b88248438111001b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25c24a81068e4768b726481ac6f19f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd470b8b069149849e2241f2e7ff8a37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77ee48942e644b1abbfaae905945fd74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a940d49cfef4fecbe0b68cb1b9dd83b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4d0d4cf11bf4d1687c63797abec16ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "123b7e303dae4461bef89de9b5ab0e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8679fa1627f343748b406688b56dace6",
              "IPY_MODEL_a09f2185fce641d29569f1f961c3e1e8",
              "IPY_MODEL_3ca7b790a5f846b2a54d52f16d152efc"
            ],
            "layout": "IPY_MODEL_aea41f7fc597493ab9d71b422ee649f4"
          }
        },
        "8679fa1627f343748b406688b56dace6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25e70cf453244ff994d17f7eadf2e394",
            "placeholder": "​",
            "style": "IPY_MODEL_f7fcb46e6bda4d2daaf121557d90ede7",
            "value": "Generating validation split: 100%"
          }
        },
        "a09f2185fce641d29569f1f961c3e1e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6775ad09e7e747e3ab2ad75a7c4fd3a6",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73b2f08bb63544aebb2fcbd7da83a652",
            "value": 1000
          }
        },
        "3ca7b790a5f846b2a54d52f16d152efc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_247306256f1d46c1ac1fdb3100999081",
            "placeholder": "​",
            "style": "IPY_MODEL_071a49de62bb472a818e7bf485dcaf53",
            "value": " 1000/1000 [00:00&lt;00:00, 34937.10 examples/s]"
          }
        },
        "aea41f7fc597493ab9d71b422ee649f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25e70cf453244ff994d17f7eadf2e394": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7fcb46e6bda4d2daaf121557d90ede7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6775ad09e7e747e3ab2ad75a7c4fd3a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73b2f08bb63544aebb2fcbd7da83a652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "247306256f1d46c1ac1fdb3100999081": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "071a49de62bb472a818e7bf485dcaf53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d368bd5fee8549da922bc1d0155bbd4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_889189d8b9c54e3f94ddb52f9a450433",
              "IPY_MODEL_2fcecbfde21642fd97d1c2334913e1ab",
              "IPY_MODEL_532feb8817a04627abf30b116d324180"
            ],
            "layout": "IPY_MODEL_c11acdb82a1440138192a6b90bc4347a"
          }
        },
        "889189d8b9c54e3f94ddb52f9a450433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4c7c656fb4d4ead9d0187c41228b926",
            "placeholder": "​",
            "style": "IPY_MODEL_cbf2f457ff5f41b1839d596c60feb88d",
            "value": "Map: 100%"
          }
        },
        "2fcecbfde21642fd97d1c2334913e1ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb952396e4e548488b6515c04ac90a14",
            "max": 13232,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d13e85014d9a417fab1ae937c4859364",
            "value": 13232
          }
        },
        "532feb8817a04627abf30b116d324180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7145419fa104564a861ef0153151d5b",
            "placeholder": "​",
            "style": "IPY_MODEL_fea196d3ac4a46ee994d8dc21f3c8e6d",
            "value": " 13232/13232 [00:00&lt;00:00, 18955.72 examples/s]"
          }
        },
        "c11acdb82a1440138192a6b90bc4347a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c7c656fb4d4ead9d0187c41228b926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbf2f457ff5f41b1839d596c60feb88d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb952396e4e548488b6515c04ac90a14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d13e85014d9a417fab1ae937c4859364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7145419fa104564a861ef0153151d5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fea196d3ac4a46ee994d8dc21f3c8e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}